{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-1e9fd325078d>:13: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "# Import the required packages\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "pd.set_option('display.max_rows', 20000)\n",
    "pd.set_option('display.max_columns', 20000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flot_to_str(obj):\n",
    "    \"\"\"\n",
    "    Convert Upc code from float to string.\n",
    "    Use this function by applying lambda\n",
    "    Parameters: \"Upc\" column of DataFrame\n",
    "    Return:string converted Upc removing dot\n",
    "    \"\"\"\n",
    "    while obj != 'np.nan':\n",
    "        obj = str(obj).split('.')[0]\n",
    "        if len(obj) == 10:\n",
    "            obj = obj + '0'\n",
    "        elif len(obj) == 4:\n",
    "            obj = obj + '0000000' \n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company(upcData):\n",
    "    \"\"\"\n",
    "    Return company code from given Upc code.\n",
    "    Parameters:'Upc' column of DataFrame\n",
    "    Return: company code\n",
    "    \"\"\"\n",
    "    try:\n",
    "        code = upcData[: 6]\n",
    "        if code == '000000':\n",
    "            return x[-5]\n",
    "        return code\n",
    "    except:\n",
    "        return -9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prodct(upcData):\n",
    "    \"\"\"\n",
    "    Return company code from given Upc code.\n",
    "    Parameters:'Upc' column of DataFrame\n",
    "    Return: company code\n",
    "    \"\"\"\n",
    "    try:\n",
    "        code = upcData[6 :]\n",
    "        return code\n",
    "    except:\n",
    "        return -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df = df.drop([\"Upc\", \"FinelineNumber\"], axis=1)\n",
    "\n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "\n",
    "    # now we add the groupby values\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()\n",
    "    \n",
    "    # finally, we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"], dummy_na=True)\n",
    "\n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "\n",
    "    return X, y, XX, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, XX, yy = transform_data(\"../data/train.csv\", \"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.78      0.81      0.79       794\n",
      "           4       0.19      0.15      0.17        67\n",
      "           5       0.71      0.77      0.74       976\n",
      "           6       0.71      0.63      0.67       260\n",
      "           7       0.62      0.65      0.63      1205\n",
      "           8       0.74      0.80      0.77      2540\n",
      "           9       0.70      0.68      0.69      1968\n",
      "          12       0.29      0.04      0.07        49\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.52      0.35      0.42       204\n",
      "          18       0.45      0.33      0.38       121\n",
      "          19       0.35      0.23      0.27        75\n",
      "          20       0.60      0.60      0.60       130\n",
      "          21       0.62      0.62      0.62       155\n",
      "          22       0.45      0.32      0.37       206\n",
      "          23       0.36      0.31      0.33        26\n",
      "          24       0.58      0.48      0.52       581\n",
      "          25       0.64      0.71      0.67       793\n",
      "          26       0.49      0.34      0.40       104\n",
      "          27       0.63      0.58      0.60       164\n",
      "          28       0.50      0.36      0.42       112\n",
      "          29       0.13      0.07      0.09        83\n",
      "          30       0.46      0.33      0.38       232\n",
      "          31       0.57      0.53      0.55       123\n",
      "          32       0.67      0.70      0.68       433\n",
      "          33       0.61      0.52      0.56       275\n",
      "          34       0.54      0.55      0.55       134\n",
      "          35       0.50      0.57      0.53       411\n",
      "          36       0.55      0.63      0.59       635\n",
      "          37       0.56      0.46      0.51       617\n",
      "          38       0.49      0.31      0.38       604\n",
      "          39       0.51      0.73      0.60      2103\n",
      "          40       0.71      0.94      0.81      1217\n",
      "          41       0.00      0.00      0.00       124\n",
      "          42       0.35      0.09      0.14       371\n",
      "          43       0.10      0.01      0.01       168\n",
      "          44       0.45      0.02      0.04       249\n",
      "         999       0.84      0.77      0.81      1799\n",
      "\n",
      "    accuracy                           0.65     20109\n",
      "   macro avg       0.50      0.45      0.46     20109\n",
      "weighted avg       0.63      0.65      0.63     20109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "clf = ensemble.RandomForestClassifier(random_state=2)\n",
    "clf.fit(X_train, y_train);\n",
    "#rf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "#predictions = clf.predict(X_train)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_valid, clf.predict(X_valid)))\n",
    "\n",
    "#param_grid = { \"criterion\" : [\"gini\", \"entropy\"]}\n",
    "\n",
    "#tree_clf = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "\n",
    "#tree_clf.fit(X_train, y_train)\n",
    "#best_tree_clf = tree_clf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest accuracy:  0.6807971014492754\n",
      "RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1)\n",
      "The best classifier so far is: \n",
      "RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=('clf', 'best_acc'))\n",
    "print('Best Random Forest accuracy: ', tree_clf.best_score_)\n",
    "print(best_tree_clf)\n",
    "results = results.append({'clf': best_tree_clf, 'best_acc': tree_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_acc'].idxmax()]['clf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67029, 230), (28645, 230))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yy = results.clf.iloc[0].predict(XX)\n",
    "yy = clf.predict(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(XX.VisitNumber, yy)), columns=[\"VisitNumber\", \"TripType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"randomForest_baseline.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mentoria] *",
   "language": "python",
   "name": "conda-env-mentoria-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
